{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "}# BERT Model - Prediction Notebook\n",
        "\n",
        "Notebook này dùng để load model BERT đã được train và dự đoán với các emails mới.\n",
        "\n",
        "## Chức năng:\n",
        "1. Load model BERT đã được train\n",
        "2. Load tokenizer\n",
        "3. Dự đoán với mảng emails mới\n",
        "4. Hiển thị kết quả dự đoán\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Setup và Mount Google Drive\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mgoogle\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcolab\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[32m      2\u001b[39m drive.mount(\u001b[33m'\u001b[39m\u001b[33m/content/drive\u001b[39m\u001b[33m'\u001b[39m)\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Import Thư Viện\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPORTS\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Transformers\n",
        "from transformers import (\n",
        "    BertTokenizerFast,\n",
        "    BertForSequenceClassification\n",
        ")\n",
        "\n",
        "print(\"✓ Tất cả thư viện đã được import thành công!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Cấu Hình Đường Dẫn\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CONFIGURATION\n",
        "\n",
        "BASE_PATH = '/content/drive/MyDrive/Colab Notebooks/Graduation-Project'\n",
        "MODEL_PATH = f\"{BASE_PATH}/outputs/models/BERT/bert_base_email_model\"\n",
        "\n",
        "print(f\"✓ Working directory: {BASE_PATH}\")\n",
        "print(f\"✓ Model path: {MODEL_PATH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Load Model và Tokenizer\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# LOAD MODEL AND TOKENIZER\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ĐANG LOAD MODEL VÀ TOKENIZER\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load tokenizer\n",
        "print(f\"Đang load tokenizer từ: {MODEL_PATH}...\")\n",
        "tokenizer = BertTokenizerFast.from_pretrained(MODEL_PATH)\n",
        "print(\"✓ Tokenizer loaded\")\n",
        "\n",
        "# Load model\n",
        "print(f\"Đang load model từ: {MODEL_PATH}...\")\n",
        "model = BertForSequenceClassification.from_pretrained(MODEL_PATH)\n",
        "print(\"✓ Model loaded\")\n",
        "\n",
        "# Set model to evaluation mode\n",
        "model.eval()\n",
        "print(\"✓ Model set to evaluation mode\")\n",
        "\n",
        "# Determine device (CPU or GPU)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model = model.to(device)\n",
        "print(f\"✓ Model moved to device: {device}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Định Nghĩa Hàm Dự Đoán\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def predict_emails(model, tokenizer, emails, max_length=512, device=None):\n",
        "    \"\"\"\n",
        "    Dự đoán labels cho một mảng emails.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model đã được load\n",
        "        tokenizer: BERT tokenizer đã được load\n",
        "        emails: List hoặc array chứa các email texts (strings)\n",
        "        max_length: Độ dài tối đa của sequence (default: 512)\n",
        "        device: Device để chạy model (None = tự động detect)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (predicted_labels, probabilities)\n",
        "            - predicted_labels: numpy array chứa labels (0 = Benign, 1 = Phishing)\n",
        "            - probabilities: numpy array chứa probabilities cho class 1 (Phishing)\n",
        "    \"\"\"\n",
        "    # Set model to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    # Auto-detect device if not provided\n",
        "    if device is None:\n",
        "        device = next(model.parameters()).device\n",
        "    \n",
        "    # Convert emails to list if it's not already\n",
        "    if not isinstance(emails, list):\n",
        "        emails = list(emails)\n",
        "    \n",
        "    # Tokenize emails\n",
        "    encodings = tokenizer(\n",
        "        emails,\n",
        "        truncation=True,\n",
        "        max_length=max_length,\n",
        "        padding=True,\n",
        "        return_tensors=\"pt\",\n",
        "    )\n",
        "    \n",
        "    # Move input tensors to the same device as the model\n",
        "    encodings = {key: val.to(device) for key, val in encodings.items()}\n",
        "    \n",
        "    # Make predictions\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**encodings)\n",
        "        logits = outputs.logits\n",
        "        \n",
        "        # Get predicted labels (0 or 1)\n",
        "        predicted_labels = torch.argmax(logits, dim=1).cpu().numpy()\n",
        "        \n",
        "        # Get probabilities (softmax)\n",
        "        probabilities = torch.softmax(logits, dim=1).cpu().numpy()\n",
        "        \n",
        "        # Extract probability of class 1 (Phishing)\n",
        "        phishing_probs = probabilities[:, 1]\n",
        "    \n",
        "    return predicted_labels, phishing_probs\n",
        "ss\n",
        "\n",
        "def predict_single_email(model, tokenizer, email, max_length=512, device=None):\n",
        "    \"\"\"\n",
        "    Dự đoán label cho một email đơn lẻ.\n",
        "\n",
        "    Args:\n",
        "        model: BERT model đã được load\n",
        "        tokenizer: BERT tokenizer đã được load\n",
        "        email: Email text (string)\n",
        "        max_length: Độ dài tối đa của sequence (default: 512)\n",
        "        device: Device để chạy model (None = tự động detect)\n",
        "\n",
        "    Returns:\n",
        "        tuple: (label, probability)\n",
        "            - label: 0 (Benign) hoặc 1 (Phishing)\n",
        "            - probability: Probability của class Phishing\n",
        "    \"\"\"\n",
        "    labels, probs = predict_emails(model, tokenizer, [email], max_length, device)\n",
        "    return labels[0], probs[0]\n",
        "\n",
        "print(\"✓ Hàm dự đoán đã được định nghĩa\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Dự Đoán với Emails Mới\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TẠO MẢNG EMAILS MỚI ĐỂ DỰ ĐOÁN\n",
        "\n",
        "new_emails = [\n",
        "    \"Internal HR Data Alignment Notice. This message is part of our quarterly internal alignment initiative focused on ensuring consistency across employee records following recent system integrations. As several tools were consolidated (time tracking, internal directory, notification preferences), we identified that a subset of profiles may show partial mismatches due to legacy formatting or delayed synchronization. This is not a corrective action and does not imply an issue with your account. However, to prevent potential delays in access provisioning or automated approvals, employees are asked to complete a brief self-review. The review confirms that basic profile attributes—such as display name, team affiliation, internal contact reference, and notification routing—are still accurate. The process is intentionally lightweight and should take no more than a few minutes. If no changes are required, you can simply confirm the current state. If updates are needed, you may annotate them directly so the system can reconcile differences without creating a support ticket. To proceed, please access the internal portal at [PORTAL], navigate to Profile Review, and complete the highlighted sections. Once reviewed, select [ACTION] to submit confirmation. If your profile has already been verified recently, the system will automatically mark it as complete and no further steps will be required. Please note that unconfirmed profiles may temporarily be placed into a pending state as part of automated compliance checks. This does not restrict general access but may delay role-specific permissions until reconciliation is complete. If you encounter any difficulties or believe this notice was sent in error, feel free to reply to this message so we can assist promptly. Thank you for helping us maintain accurate internal records.\",\n",
        "    \"Logistics Intake Confirmation Hello, The office intake desk has received an item that appears to be associated with an internal shipment but lacks a complete recipient record. The label includes an abbreviated name and internal reference code, which matches multiple profiles. To prevent misrouting or automatic return, we are contacting potential recipients to confirm ownership. This is a standard procedure for unidentified deliveries and does not indicate any issue. If you are expecting equipment, documents, or materials, please confirm whether this item belongs to you. Confirmation requires only minimal information to route the package correctly. Visit [OFFICE_PORTAL], navigate to Deliveries, enter reference [ID], and select [ACTION]. If the item is not yours, you may select “Not my delivery,” and the system will continue matching. Items without confirmation are held for a limited time per policy before being returned to the sender. We apologize for the interruption and appreciate your quick response to help resolve the intake efficiently.\",\n",
        "    \"IT Security Verification (Policy-Compliant Language) Hello, As part of our ongoing security posture review, the IT team is conducting a controlled verification of account metadata following recent environment updates. This review focuses on identifying benign anomalies such as new browser fingerprints, network changes, or device reconfigurations that can trigger precautionary flags. Your account was included in this review due to a non-critical change pattern detected last week. This does not indicate unauthorized activity. The verification process simply ensures the system correctly associates recent activity with a trusted user context. No information is collected via email. The only required step is to access the internal security console and confirm the listed device and session attributes. If the information matches your usage, the verification will complete immediately. Please visit [SECURITY_PORTAL], open Session Review, and follow the on-screen instructions. Once complete, select [ACTION] to finalize. If the review is not completed within the verification window, the system may temporarily limit certain sensitive operations (such as privilege elevation or environment exports) until confirmation is received. If you have already completed a similar check recently, the portal will reflect a completed status and no further action is required. For questions or access issues, reply to this message so we can assist. Thank you for helping maintain a secure and reliable environment.\",\n",
        "    \"Shared Documentation Access Normalization Hello, We are in the process of consolidating shared project documentation into a standardized structure to reduce duplication and ensure everyone is referencing the most current materials. As part of this effort, legacy permissions are being reviewed and normalized. During this transition, some users may notice that previously accessible documents now appear with a “request access” status. This is expected behavior while roles are revalidated against the updated access model. To streamline this process and avoid manual requests, we are asking users to confirm their current role and access level. This allows the system to automatically assign appropriate permissions without over-provisioning, which is required for audit compliance. Please go to [PORTAL], select Access Review, verify your role within the relevant project space, and submit using [ACTION]. If your role does not require edit permissions, selecting view-only will still ensure uninterrupted access. If you believe your access has been incorrectly restricted or if you require temporary elevated access, respond to this email and we will review it manually. Our goal is to complete this normalization quickly so everyone can continue working without disruption.\",\n",
        "    \"Billing Clarification and Cost Attribution Review Hello, We are reaching out as part of the routine end-of-cycle reconciliation for operational expenses. During aggregation, a small number of entries were flagged due to missing or ambiguous cost attribution tags. This is common when services are renewed, reclassified, or split across initiatives. Your name appears as the reference contact for one of the affected entries. This does not indicate an error; it simply means the system requires confirmation to finalize allocation and prevent the item from being deferred to the next reporting period. To simplify the process, we have prepared a summarized view that excludes sensitive payment details and focuses only on descriptive fields: service category, usage window, project reference, and cost center. If the information appears correct, you may approve it directly. If adjustments are required, selecting the adjustment option will allow you to leave a brief clarification note. Please log into [PORTAL], open Billing Review, locate reference [ID], and select [ACTION] to complete the confirmation. If no action is taken before the cycle closes, the item may temporarily remain pending, which could affect monthly summaries but will not impact approvals already granted. We understand end-of-period workloads can be heavy, so this message is intended to provide context and reduce follow-up. Let us know if you need assistance or if this reference should be reassigned.\",\n",
        "    \"Security Training Simulation Announcement Hello PTITer, This week we are running a scheduled security awareness simulation designed to improve recognition of suspicious messages and reduce response time to potential threats. As part of the exercise, participants may receive simulated emails that resemble phishing attempts, including urgent language or verification prompts. Please be aware that these messages are part of a controlled training environment. They will not contain real links, credential requests, or data collection mechanisms. The purpose is to evaluate decision-making and reporting behavior, not to test compliance through deception. If you receive a simulation message, do not follow the instructions in the \",\n",
        "    \"Research Consent Email (Contains Sensitive Vocabulary) Hello, You are invited to participate in an internal research study examining how users interpret system notifications that reference identity verification, account confirmation, or credential-related terminology. These terms are used descriptively within hypothetical scenarios and do not require you to provide real credentials at any point.The study has been reviewed and approved through the appropriate ethics process. Participation is voluntary, and all responses are anonymized. You may stop at any time without penalty.During the study, you will review example messages and indicate how you would respond. Some messages intentionally include language often associated with phishing to evaluate perception and awareness. If you agree to participate, please visit [RESEARCH_PORTAL] and indicate consent. If you prefer not to participate, you may ignore this message.\",\n",
        "    \"Password Reset Training Module Hello, As part of mandatory onboarding training, you will complete a module that demonstrates common password reset workflows. The module includes simulated login screens and reset prompts to illustrate how legitimate systems differ from malicious ones. Do not enter real credentials during training. All inputs use mock data and exist solely within the learning environment. To begin, visit [TRAINING_PORTAL] and open the Account Safety module. Completion will be automatically recorded.\",\n",
        "    \"Finance Reminder with “Urgent” Terminology Hello, This is a required reminder regarding an outstanding receipt confirmation associated with a previously approved expense. System templates label this notice as “urgent” to ensure timely processing before the reporting window closes. No payment details or login information are requested. The confirmation simply verifies that services or goods were received as recorded. Please access [BILLING_PORTAL], locate the pending item, and submit confirmation. If you are not the correct contact, forwarding this message to the appropriate party will help avoid delays. We appreciate your attention to this administrative requirement.\",\n",
        "    \"QA Report Discussing Phishing Data Hello team, Attached is this week’s QA summary covering dataset quality for email classification tasks. The report includes analysis of phishing indicators, credential language, and attack-style phrasing found in sample data. These references describe dataset content only and do not represent real incidents. The goal is to improve model robustness by understanding why certain benign emails are misclassified and vice versa. Please review and share any observations that could help refine preprocessing or labeling standards.\",\n",
        "]\n",
        "\n",
        "print(f\"✓ Đã tạo {len(new_emails)} emails để dự đoán\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DỰ ĐOÁN VỚI MẢNG EMAILS\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ĐANG DỰ ĐOÁN VỚI MẢNG EMAILS\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Make predictions\n",
        "predicted_labels, probabilities = predict_emails(model, tokenizer, new_emails)\n",
        "\n",
        "print(f\"\\n✓ Đã dự đoán {len(predicted_labels)} emails\")\n",
        "print(f\"✓ Labels: {predicted_labels}\")\n",
        "print(f\"✓ Probabilities: {probabilities}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HIỂN THỊ KẾT QUẢ CHI TIẾT\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"KẾT QUẢ DỰ ĐOÁN CHI TIẾT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "for i, email in enumerate(new_emails, 1):\n",
        "    label = 'Phishing' if predicted_labels[i-1] == 1 else 'Benign'\n",
        "    prob = probabilities[i-1]\n",
        "    \n",
        "    print(f\"\\n[Email {i}]\")\n",
        "    print(f\"  Text: {email[:80]}{'...' if len(email) > 80 else ''}\")\n",
        "    print(f\"  Predicted Label: {label}\")\n",
        "    print(f\"  Probability (Phishing): {prob:.4f}\")\n",
        "    print(f\"  Confidence: {'High' if prob > 0.9 or prob < 0.1 else 'Medium' if prob > 0.7 or prob < 0.3 else 'Low'}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Dự Đoán với Email Đơn Lẻ (Ví dụ)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DỰ ĐOÁN VỚI MỘT EMAIL ĐƠN LẺ\n",
        "\n",
        "single_email = \"Your account has been suspended. Please click here to verify your identity: http://verify-account.com\"\n",
        "\n",
        "label, prob = predict_single_email(model, tokenizer, single_email)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"DỰ ĐOÁN EMAIL ĐƠN LẺ\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Email: {single_email}\")\n",
        "print(f\"Predicted Label: {'Phishing' if label == 1 else 'Benign'}\")\n",
        "print(f\"Probability (Phishing): {prob:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Tóm Tắt Kết Quả\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TÓM TẮT KẾT QUẢ\n",
        "\n",
        "benign_count = np.sum(predicted_labels == 0)\n",
        "phishing_count = np.sum(predicted_labels == 1)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TÓM TẮT KẾT QUẢ\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Tổng số emails: {len(new_emails)}\")\n",
        "print(f\"Benign: {benign_count} ({benign_count/len(new_emails)*100:.1f}%)\")\n",
        "print(f\"Phishing: {phishing_count} ({phishing_count/len(new_emails)*100:.1f}%)\")\n",
        "print(f\"\\nAverage probability (Phishing): {np.mean(probabilities):.4f}\")\n",
        "print(f\"Min probability: {np.min(probabilities):.4f}\")\n",
        "print(f\"Max probability: {np.max(probabilities):.4f}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
