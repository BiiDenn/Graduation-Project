"""
Module SHAP (SHapley Additive exPlanations) cho BERT.

M√¥-ƒëun n√†y tri·ªÉn khai m·ªôt c√°ch gi·∫£i th√≠ch d·∫°ng "SHAP-like" b·∫±ng k·ªπ thu·∫≠t
mask t·ª´ng token v√† ƒëo ƒë·ªô thay ƒë·ªïi x√°c su·∫•t/logit, ƒë∆∞·ª£c t·ªëi ∆∞u ƒë·ªÉ d√πng
trong extension tr√¨nh duy·ªát (kho·∫£ng v√†i gi√¢y cho m·ªói email).
"""

from __future__ import annotations

import threading
import time
import warnings
import re
from typing import Any, Dict, Iterable, List, Optional

import numpy as np
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer

warnings.filterwarnings("ignore")


class SHAPExplainer:
    """
    T·∫°o gi·∫£i th√≠ch d·∫°ng SHAP cho c√°c model BERT-based (HuggingFace).

    √ù t∆∞·ªüng ch√≠nh:
    - Cache model/tokenizer, ch·ªâ load 1 l·∫ßn (thread‚Äësafe).
    - D√πng batching khi ch·∫°y c√°c phi√™n b·∫£n ƒë√£ mask token.
    - Gom v√† l·ªçc c√°c token ƒë·ªÉ l·∫•y danh s√°ch t·ª´ kh√≥a d·ªÖ ƒë·ªçc cho ng∆∞·ªùi d√πng.
    """

    def __init__(self, model_path: str, device: Optional[str] = None, preload_model: bool = True) -> None:
        """
        Args:
            model_path: Th∆∞ m·ª•c ch·ª©a model HuggingFace (config.json, tokenizer, weights...).
            device: `'cuda'`, `'cpu'` ho·∫∑c None ƒë·ªÉ t·ª± ƒë·ªông ch·ªçn.
            preload_model: N·∫øu True, load model ngay khi kh·ªüi t·∫°o (trong background thread).
        """
        self.model_path = model_path
        self.device = device or ("cuda" if torch.cuda.is_available() else "cpu")

        # Cache / tr·∫°ng th√°i load
        self._model: Optional[torch.nn.Module] = None
        self._tokenizer: Optional[AutoTokenizer] = None
        self._explainer: Any = None  # Gi·ªØ l·∫°i thu·ªôc t√≠nh n√†y ƒë·ªÉ kh√¥ng ph√° v·ª° API c≈© n·∫øu c√≥ d√πng reflection
        self._model_loading: bool = False
        self._model_lock = threading.Lock()

        # Gi·ªõi h·∫°n ƒë·ªô d√†i chu·ªói ƒë·∫ßu v√†o theo config model
        self._max_length = self._get_max_length_from_config()

        print(f"üîß SHAPExplainer initialized (device: {self.device}, max_length: {self._max_length})")

        # Preload model trong background ƒë·ªÉ l·∫ßn g·ªçi ƒë·∫ßu kh√¥ng b·ªã tr·ªÖ
        if preload_model:
            print(f"üì• Pre-loading model t·ª´ {model_path} trong background...")
            self._load_thread = threading.Thread(target=self._load_model, daemon=True)
            self._load_thread.start()
            # Cho background thread m·ªôt ch√∫t th·ªùi gian ƒë·ªÉ b·∫Øt ƒë·∫ßu
            time.sleep(0.1)

    # --------------------------------------------------------------------- #
    # Model loading helpers
    # --------------------------------------------------------------------- #
    def _get_max_length_from_config(self) -> int:
        """
        L·∫•y `max_length` h·ª£p l√Ω t·ª´ file `config.json` c·ªßa model.

        Returns:
            ƒê·ªô d√†i t·ªëi ƒëa cho tokenizer (ƒë√£ tr·ª´ 2 token ƒë·∫∑c bi·ªát CLS/SEP).
            - N·∫øu kh√¥ng ƒë·ªçc ƒë∆∞·ª£c config: 512.
        """
        import json
        import os

        config_path = os.path.join(self.model_path, "config.json")
        if os.path.exists(config_path):
            try:
                with open(config_path, "r", encoding="utf-8") as f:
                    config = json.load(f)

                max_position_embeddings = config.get("max_position_embeddings", 512)
                # Tr·ª´ 2 cho special tokens (CLS v√† SEP)
                return max_position_embeddings - 2
            except Exception as exc:  # gi·ªØ h√†nh vi c≈©: log v√† fallback
                print(f"‚ö† Kh√¥ng th·ªÉ ƒë·ªçc config, s·ª≠ d·ª•ng max_length m·∫∑c ƒë·ªãnh 512: {exc}")
                return 512

        # Kh√¥ng c√≥ config, d√πng m·∫∑c ƒë·ªãnh cho BERT
        return 512

    def _load_model(self) -> None:
        """
        Load model & tokenizer m·ªôt l·∫ßn duy nh·∫•t (thread‚Äësafe).
        """
        # Ch·∫∑n c√°c thread kh√°c n·∫øu ƒëang load
        with self._model_lock:
            if self._model is not None and self._tokenizer is not None:
                return
            if self._model_loading:
                return
            self._model_loading = True

        try:
            print(f"üì• Loading BERT model from {self.model_path}...")
            start_time = time.time()

            tokenizer = AutoTokenizer.from_pretrained(
                self.model_path,
                use_fast=True,
            )

            model = AutoModelForSequenceClassification.from_pretrained(
                self.model_path,
                torch_dtype=torch.float32,
            )
            model.to(self.device)
            model.eval()

            # Warm‚Äëup model v·ªõi dummy input (gi√∫p l·∫ßn ch·∫°y th·∫≠t nhanh ·ªïn ƒë·ªãnh h∆°n)
            try:
                dummy_inputs = tokenizer(
                    "test",
                    padding=True,
                    truncation=True,
                    max_length=min(128, self._max_length),
                    return_tensors="pt",
                )
                dummy_inputs = {k: v.to(self.device) for k, v in dummy_inputs.items()}
                with torch.no_grad():
                    _ = model(**dummy_inputs)
            except Exception as warmup_error:
                print(f"‚ö† Warm-up model th·∫•t b·∫°i (kh√¥ng ·∫£nh h∆∞·ªüng): {warmup_error}")

            # Ghi l·∫°i v√†o instance sau khi load xong ho√†n to√†n
            with self._model_lock:
                self._tokenizer = tokenizer
                self._model = model
                self._model_loading = False

            print(f"‚úì Model loaded in {time.time() - start_time:.2f}s")
        except Exception as exc:
            with self._model_lock:
                self._model_loading = False
            print(f"‚ùå L·ªói khi load model: {exc}")
            raise

    def _wait_until_model_ready(self) -> None:
        """
        ƒê·∫£m b·∫£o model/tokenizer ƒë√£ s·∫µn s√†ng, ch·ªù background thread n·∫øu c·∫ßn.
        """
        self._load_model()

        if hasattr(self, "_load_thread") and self._load_thread.is_alive():
            print("‚è≥ ƒêang ƒë·ª£i model load xong...")
            self._load_thread.join(timeout=60)
            if self._model is None:
                raise RuntimeError("Model kh√¥ng th·ªÉ load ƒë∆∞·ª£c trong th·ªùi gian cho ph√©p")

        if self._model is None or self._tokenizer is None:
            raise RuntimeError("Model ch∆∞a ƒë∆∞·ª£c load. Vui l√≤ng ƒë·ª£i th√™m ho·∫∑c ki·ªÉm tra l·∫°i.")

    # --------------------------------------------------------------------- #
    # Prediction helpers
    # --------------------------------------------------------------------- #
    def _create_predict_fn(self):
        """
        Tr·∫£ v·ªÅ h√†m nh·∫≠n list string v√† tr·∫£ v·ªÅ x√°c su·∫•t \\(shape = (n_samples, 2)\\).
        """
        self._wait_until_model_ready()

        def predict_fn(texts: Iterable[str]) -> np.ndarray:
            # ƒê·∫£m b·∫£o ƒë·∫ßu v√†o l√† list[str]
            if isinstance(texts, str):
                texts = [texts]
            elif not isinstance(texts, (list, tuple)):
                texts = [str(texts)]

            assert self._tokenizer is not None  # ƒë·ªÉ mypy/IDE h√†i l√≤ng
            inputs = self._tokenizer(
                list(texts),
                padding=True,
                truncation=True,
                max_length=self._max_length,
                return_tensors="pt",
            )
            inputs = {k: v.to(self.device) for k, v in inputs.items()}

            assert self._model is not None
            with torch.no_grad():
                outputs = self._model(**inputs)
                logits = outputs.logits
                probabilities = torch.softmax(logits, dim=-1).cpu().numpy()

            # BERT output: [prob_class_0, prob_class_1] (benign, phishing)
            return probabilities

        return predict_fn

    # --------------------------------------------------------------------- #
    # Core explanation logic
    # --------------------------------------------------------------------- #
    def explain_with_shap(
        self,
        email_text: str,
        num_samples: int = 50,  # Gi·ªØ l·∫°i tham s·ªë cho backward‚Äëcompat, kh√¥ng d√πng tr·ª±c ti·∫øp
        max_features: int = 15,
        token_limit: Optional[int] = None,
        batch_size: int = 32,
    ) -> Dict[str, Any]:
        """
        Sinh gi·∫£i th√≠ch d·∫°ng SHAP cho m·ªôt email.

        Args:
            email_text: N·ªôi dung email c·∫ßn gi·∫£i th√≠ch.
            num_samples: Tham s·ªë gi·ªØ l·∫°i ƒë·ªÉ t∆∞∆°ng th√≠ch (kh√¥ng d√πng tr·ª±c ti·∫øp n·ªØa).
            max_features: S·ªë l∆∞·ª£ng t·ª´ kh√≥a quan tr·ªçng tr·∫£ v·ªÅ.
            token_limit: Gi·ªõi h·∫°n s·ªë token ƒë·∫ßu v√†o ƒë·ªÉ tƒÉng t·ªëc (None = d√πng h·∫øt).
            batch_size: K√≠ch th∆∞·ªõc batch khi ch·∫°y c√°c b·∫£n ƒë√£ mask.
        """
        start_time = time.time()
        self._wait_until_model_ready()

        predict_fn = self._create_predict_fn()
        initial_pred = predict_fn([email_text])[0]
        prob_benign = float(initial_pred[0])
        prob_phishing = float(initial_pred[1])
        label = "phishing" if prob_phishing > 0.5 else "benign"
        probability = prob_phishing if prob_phishing > 0.5 else prob_benign

        print("üîç Generating SHAP explanation (analyzing up to 512 tokens)...")

        # Tokenize ƒë·ªÉ l·∫•y subwords
        assert self._tokenizer is not None
        bert_tokens = self._tokenizer.tokenize(email_text)
        if not bert_tokens:
            bert_tokens = email_text.split()

        # Gi·ªõi h·∫°n s·ªë token
        effective_limit = self._max_length
        if token_limit is not None:
            effective_limit = min(effective_limit, token_limit)
        bert_tokens = bert_tokens[: min(effective_limit, len(bert_tokens))]

        # Baseline: d·ª± ƒëo√°n tr√™n email g·ªëc
        baseline_pred = predict_fn([email_text])[0]
        baseline_prob_phishing = float(baseline_pred[1])

        encoded = self._tokenizer.encode_plus(
            email_text,
            return_tensors="pt",
            padding=True,
            truncation=True,
            max_length=self._max_length,
        )
        original_input_ids = encoded["input_ids"].clone()
        attention_mask = encoded["attention_mask"]
        mask_token_id = self._tokenizer.mask_token_id

        shap_values_simple: List[Dict[str, float]] = []
        masked_inputs_list: List[torch.Tensor] = []
        token_meta: List[str] = []

        # T·∫°o b·∫£n masked cho t·ª´ng token
        for token in bert_tokens:
            token_clean = token.replace("##", "").strip()
            if not token_clean:
                continue

            try:
                token_id = self._tokenizer.convert_tokens_to_ids([token])[0]
                if token_id == self._tokenizer.unk_token_id:
                    token_id = self._tokenizer.convert_tokens_to_ids([token_clean])[0]
            except Exception:
                # Kh√¥ng map ƒë∆∞·ª£c token ‚Üí b·ªè qua
                continue

            masked_input_ids = original_input_ids.clone()
            mask_positions = (masked_input_ids[0] == token_id).nonzero(as_tuple=True)[0]
            if not len(mask_positions):
                continue

            # Ch·ªâ mask token ƒë·∫ßu ti√™n kh√¥ng ph·∫£i CLS/SEP
            masked = False
            for pos in mask_positions:
                if 0 < pos < len(masked_input_ids[0]) - 1:
                    masked_input_ids[0][pos] = mask_token_id
                    masked = True
                    break

            if masked:
                masked_inputs_list.append(masked_input_ids)
                token_meta.append(token_clean or token)

        if masked_inputs_list:
            baseline_logit_phishing = float(
                torch.logit(torch.tensor(baseline_prob_phishing), eps=1e-6)
            )

            idx = 0
            while idx < len(masked_inputs_list):
                batch_inputs_ids = masked_inputs_list[idx : idx + batch_size]
                batch_tokens = token_meta[idx : idx + batch_size]

                batch_input_ids_tensor = torch.cat(batch_inputs_ids, dim=0).to(self.device)
                batch_attention_mask = attention_mask.repeat(
                    batch_input_ids_tensor.size(0), 1
                ).to(self.device)

                inputs = {
                    "input_ids": batch_input_ids_tensor,
                    "attention_mask": batch_attention_mask,
                }

                try:
                    assert self._model is not None
                    with torch.no_grad():
                        outputs = self._model(**inputs)
                        logits = outputs.logits
                        probs = torch.softmax(logits, dim=-1)

                    for j in range(batch_input_ids_tensor.size(0)):
                        masked_prob_phishing = float(probs[j][1])
                        masked_logit_phishing = float(logits[j][1])

                        delta_logit = baseline_logit_phishing - masked_logit_phishing
                        delta_prob = baseline_prob_phishing - masked_prob_phishing

                        shap_value = delta_logit if abs(delta_logit) > 0.01 else delta_prob * 1000

                        shap_values_simple.append(
                            {
                                "token": batch_tokens[j],
                                "weight": float(shap_value),
                            }
                        )
                except Exception:
                    # Gi·ªØ nguy√™n h√†nh vi c≈©: n·∫øu batch l·ªói th√¨ b·ªè qua batch ƒë√≥
                    pass

                idx += batch_size

        # ------------------------------------------------------------------ #
        # Gom v√† l·ªçc token
        # ------------------------------------------------------------------ #
        token_dict: Dict[str, float] = {}
        for item in shap_values_simple:
            token = item["token"]
            weight = item["weight"]
            token_dict[token] = token_dict.get(token, 0.0) + weight

        def _is_valid_word(token: str) -> bool:
            """
            Ki·ªÉm tra token c√≥ ph·∫£i l√† ‚Äút·ª´ th·∫≠t s·ª±‚Äù ƒë·ªÉ hi·ªÉn th·ªã cho ng∆∞·ªùi d√πng.
            """
            if not token or not token.strip():
                return False

            token_clean = token.strip()
            token_lower = token_clean.lower()

            if len(token_clean) < 2:
                return False

            english_stopwords = {
                "the",
                "and",
                "or",
                "but",
                "if",
                "while",
                "for",
                "on",
                "in",
                "at",
                "to",
                "from",
                "by",
                "with",
                "of",
                "as",
                "is",
                "are",
                "was",
                "were",
                "be",
                "been",
                "being",
                "a",
                "an",
                "this",
                "that",
                "these",
                "those",
                "it",
                "its",
                "into",
                "about",
                "over",
                "under",
                "up",
                "down",
                "your",
                "you",
                "we",
                "our",
                "us",
                "i",
                "me",
                "my",
            }
            if token_lower in english_stopwords:
                return False

            special_chars = [
                "/",
                ".",
                ":",
                "-",
                "!",
                ",",
                "'",
                '"',
                ";",
                "?",
                "(",
                ")",
                "[",
                "]",
                "{",
                "}",
                "=",
                "+",
                "*",
                "&",
                "%",
                "$",
                "#",
                "@",
                "^",
                "~",
                "`",
                "|",
                "\\",
            ]
            if token_clean in special_chars:
                return False

            # Cho ph√©p c√°c t·ª´ c√≥ ch·ª©a ch·ªØ c√°i, c√≥ th·ªÉ k√®m s·ªë ho·∫∑c d·∫•u '-'
            if re.match(r"^[a-zA-Z]+([-][a-zA-Z0-9]+)*$", token_clean):
                return True
            if re.match(r"^[a-zA-Z]+[0-9]*$", token_clean) or re.match(
                r"^[0-9]*[a-zA-Z]+$", token_clean
            ):
                return True
            if re.match(r"^[a-zA-Z]+$", token_clean):
                return True

            return False

        filtered_tokens = [
            {"token": token, "weight": weight}
            for token, weight in token_dict.items()
            if _is_valid_word(token)
        ]
        filtered_tokens.sort(key=lambda x: abs(x["weight"]), reverse=True)

        class SimpleSHAPValues:
            """
            ƒê·ªëi t∆∞·ª£ng ‚Äúgi·∫£‚Äù m√¥ ph·ªèng c·∫•u tr√∫c shap_values ƒë·ªÉ t∆∞∆°ng th√≠ch code c≈©.
            """

            def __init__(self, tokens_data: List[Dict[str, float]]) -> None:
                if tokens_data:
                    self.values = np.array([[t["weight"] for t in tokens_data]])
                    self.data = [t["token"] for t in tokens_data]
                else:
                    self.values = np.array([[]])
                    self.data: List[str] = []

        shap_values = SimpleSHAPValues(filtered_tokens)
        important_tokens = filtered_tokens[:max_features]

        elapsed_time = time.time() - start_time
        print(f"‚úì SHAP explanation completed in {elapsed_time:.2f}s")

        return {
            "email": email_text,
            "prediction_label": label,
            "prediction_probability": probability,
            "important_tokens": important_tokens[:max_features],
            "shap_values": shap_values,
            "elapsed_time": elapsed_time,
        }

    def explain_with_shap_fast(self, email_text: str, max_features: int = 15) -> Dict[str, Any]:
        """
        Phi√™n b·∫£n nhanh cho extension: gi·ªõi h·∫°n token ƒë·ªÉ ch·∫°y trong v√†i gi√¢y.
        """
        return self.explain_with_shap(
            email_text=email_text,
            num_samples=30,  # gi·ªØ tham s·ªë cho backward‚Äëcompat
            max_features=max_features,
            token_limit=80,
            batch_size=32,
        )

    def warmup(self, text: str = "test email for warmup", max_features: int = 5) -> None:
        """
        Ch·∫°y nhanh m·ªôt l·∫ßn explain ƒë·ªÉ warm‚Äëup model/tokenizer & kernel.
        """
        try:
            print("üî• ƒêang warm-up SHAPExplainer...")
            _ = self.explain_with_shap_fast(text, max_features=max_features)
            print("‚úì Warm-up SHAPExplainer ho√†n t·∫•t")
        except Exception as exc:
            print(
                f"‚ö† Warm-up SHAPExplainer th·∫•t b·∫°i (kh√¥ng ·∫£nh h∆∞·ªüng ƒë·∫øn ho·∫°t ƒë·ªông ch√≠nh): {exc}"
            )

