"""
Module ƒë·ªÉ load v√† qu·∫£n l√Ω c√°c m√¥ h√¨nh Deep Learning cho XAI.

H·ªó tr·ª£:
- BiLSTM.h5 / .keras
- CNN.h5
- GRU.h5 / .keras
- Hybrid_CNN_BiLSTM.h5 / .keras
- BERT (PyTorch, HuggingFace format)
"""

from __future__ import annotations

import os
from typing import Any, Dict, Tuple

import numpy as np
import tensorflow as tf

# Import PyTorch v√† Transformers cho BERT (optional)
try:
    import torch
    from transformers import AutoModelForSequenceClassification, AutoTokenizer

    PYTORCH_AVAILABLE = True
except ImportError:
    PYTORCH_AVAILABLE = False
    torch = None  # type: ignore[assignment]


class ModelLoader:
    """
    Ch·ªãu tr√°ch nhi·ªám:
    - T√¨m, load v√† l∆∞u tr·ªØ to√†n b·ªô models d√πng cho XAI.
    - Cung c·∫•p h√†m `predict()` th·ªëng nh·∫•t cho m·ªçi lo·∫°i model.
    """

    def __init__(self, base_path: str = "output/models") -> None:
        """
        Args:
            base_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn th∆∞ m·ª•c ch·ª©a c√°c m√¥ h√¨nh.
        """
        self.base_path = base_path
        self.models: Dict[str, Dict[str, Any]] = {}
        self.tokenizers: Dict[str, Any] = {}

    # ------------------------------------------------------------------ #
    # Keras helpers
    # ------------------------------------------------------------------ #
    def _reinitialize_text_vectorization(
        self, model: tf.keras.Model, model_name: str, model_path: str
    ) -> None:
        """
        Re-initialize TextVectorization layer vocabulary n·∫øu c·∫ßn.
        """
        try:
            text_vec_layer = None
            for layer in model.layers:
                if isinstance(layer, tf.keras.layers.TextVectorization):
                    text_vec_layer = layer
                    break

            if text_vec_layer is None:
                return

            vocab = text_vec_layer.get_vocabulary()
            if vocab and len(vocab) > 2:
                print(f"  TextVectorization vocabulary ƒë√£ c√≥ s·∫µn ({len(vocab)} tokens)")
                return

            # T√¨m vocabulary file: n·∫øu model trong checkpoints, t√¨m ·ªü th∆∞ m·ª•c cha
            model_dir = os.path.dirname(model_path)
            if "checkpoints" in model_dir:
                # L√™n 1 c·∫•p ƒë·ªÉ ra th∆∞ m·ª•c model g·ªëc
                model_base_dir = os.path.dirname(model_dir)
            else:
                model_base_dir = model_dir
            
            vocab_file = None
            possible_vocab_files = [
                os.path.join(model_base_dir, f"{model_name.lower()}_embedding_metadata.tsv"),
                os.path.join(model_base_dir, "hybrid_cnn_bilstm_embedding_metadata.tsv"),
                os.path.join(model_base_dir, "hybrid_embedding_metadata.tsv"),
            ]

            for vocab_path in possible_vocab_files:
                if os.path.exists(vocab_path):
                    vocab_file = vocab_path
                    break

            if vocab_file:
                print(f"  ƒêang load vocabulary t·ª´ {vocab_file}...")

                layer_config = text_vec_layer.get_config()
                max_tokens = layer_config.get("max_tokens", 20000)

                with open(vocab_file, "r", encoding="utf-8") as f:
                    words_from_file = [line.strip() for line in f if line.strip()]

                num_words_to_take = max_tokens - 2
                vocabulary = ["", "[UNK]"] + words_from_file[:num_words_to_take]

                print(
                    f"  Loaded {len(words_from_file)} words from file, "
                    f"taking first {num_words_to_take}"
                )
                text_vec_layer.set_vocabulary(vocabulary)
                print(f"  ‚úì ƒê√£ re-initialize vocabulary ({len(vocabulary)} tokens)")
            else:
                print("  ‚ö† Kh√¥ng t√¨m th·∫•y vocabulary file.")
                print("  Model c√≥ th·ªÉ kh√¥ng ho·∫°t ƒë·ªông ƒë√∫ng!")

        except Exception as exc:
            print(f"  ‚ö† L·ªói khi re-initialize TextVectorization: {exc}")
    
    def load_keras_model(self, model_name: str, model_path: str) -> tf.keras.Model:
        """
        Load m√¥ h√¨nh Keras (.h5 ho·∫∑c .keras) v·ªõi nhi·ªÅu ph∆∞∆°ng √°n fallback.

        N·∫øu g·∫∑p l·ªói (ƒë·∫∑c bi·ªát l√† encoding tr√™n Windows), h√†m s·∫Ω raise
        exception ƒë·ªÉ pipeline d·ª´ng ho√†n to√†n (gi·ªØ nguy√™n h√†nh vi c≈©).
        """
        print(f"ƒêang load m√¥ h√¨nh {model_name} t·ª´ {model_path}...")

        original_encoding = os.environ.get("PYTHONIOENCODING", None)
        os.environ["PYTHONIOENCODING"] = "utf-8"

        load_methods = [
            lambda: tf.keras.models.load_model(model_path),
            lambda: tf.keras.models.load_model(model_path, compile=False),
            lambda: tf.keras.models.load_model(model_path, safe_mode=False)
            if hasattr(tf.keras.models, "load_model")
            else None,
        ]

        last_error: Exception | None = None
        for i, load_method in enumerate(load_methods, 1):
            try:
                if load_method is None:
                    continue

                model = load_method()
                print(f"‚úì ƒê√£ load th√†nh c√¥ng m√¥ h√¨nh {model_name} (method {i})")

                if model_path.endswith(".h5"):
                    self._reinitialize_text_vectorization(model, model_name, model_path)

                if original_encoding:
                    os.environ["PYTHONIOENCODING"] = original_encoding
                elif "PYTHONIOENCODING" in os.environ:
                    del os.environ["PYTHONIOENCODING"]

                return model

            except (UnicodeDecodeError, ValueError) as exc:
                last_error = exc
                error_msg = str(exc)
                if (
                    ("codec can't decode" in error_msg or "charmap" in error_msg.lower())
                    and i < len(load_methods)
                ):
                    print(f"  Th·ª≠ method {i} th·∫•t b·∫°i, ƒëang th·ª≠ method ti·∫øp theo...")
                    continue
                break
            except Exception as exc:
                last_error = exc
                break

        if original_encoding:
            os.environ["PYTHONIOENCODING"] = original_encoding
        elif "PYTHONIOENCODING" in os.environ:
            del os.environ["PYTHONIOENCODING"]

        if last_error:
            error_msg = str(last_error)
            if "codec can't decode" in error_msg or "charmap" in error_msg.lower():
                print(
                    f"‚úó L·ªói encoding khi load m√¥ h√¨nh {model_name} "
                    f"(ƒë√£ th·ª≠ {len(load_methods)} methods):"
                )
                print(
                    "  Nguy√™n nh√¢n: File m√¥ h√¨nh ch·ª©a k√Ω t·ª± ƒë·∫∑c bi·ªát "
                    "kh√¥ng th·ªÉ decode b·∫±ng encoding m·∫∑c ƒë·ªãnh."
                )
                print("  H√†nh ƒë·ªông: Pipeline s·∫Ω D·ª™NG HO√ÄN TO√ÄN ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n.")
                print(
                    "  Gi·∫£i ph√°p: C·∫ßn re-save m√¥ h√¨nh v·ªõi encoding ƒë√∫ng "
                    "ho·∫∑c convert sang format .h5"
                )
            else:
                print(f"‚úó L·ªói khi load m√¥ h√¨nh {model_name}: {error_msg}")
            raise last_error

        raise RuntimeError(f"Kh√¥ng th·ªÉ load m√¥ h√¨nh {model_name} v·ªõi b·∫•t k·ª≥ method n√†o.")
    
    def load_pytorch_model(self, model_name: str, model_path: str) -> Tuple[Any, Any]:
        """
        Load m√¥ h√¨nh PyTorch (BERT t·ª´ HuggingFace).
        """
        if not PYTORCH_AVAILABLE:
            raise ImportError(
                "PyTorch v√† Transformers kh√¥ng c√≥ s·∫µn. "
                "C√†i ƒë·∫∑t: pip install torch transformers"
            )

        print(f"ƒêang load m√¥ h√¨nh {model_name} t·ª´ {model_path}...")

        try:
            tokenizer = AutoTokenizer.from_pretrained(model_path)

            device = "cuda" if torch.cuda.is_available() else "cpu"
            model = AutoModelForSequenceClassification.from_pretrained(model_path)
            model.to(device)
            model.eval()  # Set to evaluation mode

            print(f"‚úì ƒê√£ load th√†nh c√¥ng m√¥ h√¨nh {model_name} (device: {device})")
            return model, tokenizer

        except Exception as exc:
            error_msg = (
                f"\n{'='*80}\n"
                f"L·ªñI: Kh√¥ng th·ªÉ load m√¥ h√¨nh {model_name}\n"
                f"{'='*80}\n"
                f"Chi ti·∫øt l·ªói:\n{str(exc)}\n"
                f"{'='*80}\n"
            )
            print(error_msg)
            raise RuntimeError(f"Kh√¥ng th·ªÉ load m√¥ h√¨nh {model_name}") from exc
    
    def load_all_models(self) -> Dict[str, Any]:
        """
        Load t·∫•t c·∫£ c√°c m√¥ h√¨nh c√≥ s·∫µn.

        QUAN TR·ªåNG:
        - H√†m n√†y y√™u c·∫ßu T·∫§T C·∫¢ m√¥ h√¨nh ph·∫£i load th√†nh c√¥ng.
        - N·∫øu B·∫§T K·ª≤ m√¥ h√¨nh n√†o l·ªói, pipeline s·∫Ω D·ª™NG HO√ÄN TO√ÄN.
        """
        print(f"\nüîç Base path: {os.path.abspath(self.base_path)}")
        print(f"üîç Base path exists: {os.path.exists(self.base_path)}\n")

        models_dict: Dict[str, Dict[str, Any]] = {}
        required_models: list[tuple[str, str, str]] = []

        # BiLSTM
        bilstm_path_h5 = os.path.join(self.base_path, "BiLSTM", "bilstm_model.h5")
        bilstm_path_keras = os.path.join(
            self.base_path, "BiLSTM", "checkpoints", "bilstm_model.keras"
        )

        bilstm_path = None
        bilstm_path_h5_abs = os.path.abspath(bilstm_path_h5)
        bilstm_path_keras_abs = os.path.abspath(bilstm_path_keras)

        if os.path.exists(bilstm_path_h5):
            bilstm_path = bilstm_path_h5
            print(f"  ‚úì T√¨m th·∫•y BiLSTM.h5 t·∫°i: {bilstm_path_h5_abs}")
        elif os.path.exists(bilstm_path_keras):
            bilstm_path = bilstm_path_keras
            print(f"  ‚úì T√¨m th·∫•y BiLSTM checkpoint t·∫°i: {bilstm_path_keras_abs}")
        else:
            print("  ‚úó Kh√¥ng t√¨m th·∫•y BiLSTM t·∫°i:")
            print(f"    - {bilstm_path_h5_abs} (exists: {os.path.exists(bilstm_path_h5)})")
            print(
                f"    - {bilstm_path_keras_abs} "
                f"(exists: {os.path.exists(bilstm_path_keras)})"
            )

        if bilstm_path:
            required_models.append(("BiLSTM", bilstm_path, "keras"))
        else:
            raise FileNotFoundError(
                "Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh BiLSTM t·∫°i c√°c v·ªã tr√≠:\n"
                f"  - {bilstm_path_h5_abs}\n"
                f"  - {bilstm_path_keras_abs}\n"
                "T·∫•t c·∫£ m√¥ h√¨nh ph·∫£i c√≥ s·∫µn ƒë·ªÉ pipeline ho·∫°t ƒë·ªông."
            )

        # CNN
        cnn_path_h5 = os.path.join(self.base_path, "CNN", "cnn_model.h5")
        cnn_path_keras = os.path.join(
            self.base_path, "CNN", "checkpoints", "cnn_model.keras"
        )
        
        cnn_path = None
        if os.path.exists(cnn_path_h5):
            cnn_path = cnn_path_h5
            print(f"  ‚úì T√¨m th·∫•y CNN.h5 t·∫°i: {os.path.abspath(cnn_path_h5)}")
        elif os.path.exists(cnn_path_keras):
            cnn_path = cnn_path_keras
            print(f"  ‚úì T√¨m th·∫•y CNN checkpoint t·∫°i: {os.path.abspath(cnn_path_keras)}")
        else:
            print("  ‚úó Kh√¥ng t√¨m th·∫•y CNN t·∫°i:")
            print(f"    - {os.path.abspath(cnn_path_h5)} (exists: {os.path.exists(cnn_path_h5)})")
            print(f"    - {os.path.abspath(cnn_path_keras)} (exists: {os.path.exists(cnn_path_keras)})")
        
        if cnn_path:
            required_models.append(("CNN", cnn_path, "keras"))
        else:
            raise FileNotFoundError(
                "Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh CNN t·∫°i c√°c v·ªã tr√≠:\n"
                f"  - {os.path.abspath(cnn_path_h5)}\n"
                f"  - {os.path.abspath(cnn_path_keras)}\n"
                "T·∫•t c·∫£ m√¥ h√¨nh ph·∫£i c√≥ s·∫µn ƒë·ªÉ pipeline ho·∫°t ƒë·ªông."
            )

        # GRU
        gru_path_h5 = os.path.join(self.base_path, "GRU", "gru_model.h5")
        gru_path_fixed = os.path.join(self.base_path, "GRU", "gru_model_fixed.h5")
        gru_path_keras = os.path.join(
            self.base_path, "GRU", "checkpoints", "gru_model.keras"
        )

        gru_path = None
        if os.path.exists(gru_path_h5):
            gru_path = gru_path_h5
            print("  L∆∞u √Ω: S·ª≠ d·ª•ng file .h5 (∆∞u ti√™n ƒë·ªÉ tr√°nh l·ªói encoding)")
        elif os.path.exists(gru_path_fixed):
            gru_path = gru_path_fixed
            print(f"  L∆∞u √Ω: S·ª≠ d·ª•ng file fixed: {gru_path_fixed}")
        elif os.path.exists(gru_path_keras):
            gru_path = gru_path_keras
            print(
                "  C·∫£nh b√°o: ƒêang load file .keras - c√≥ th·ªÉ g·∫∑p l·ªói encoding tr√™n Windows"
            )

        if gru_path:
            required_models.append(("GRU", gru_path, "keras"))
        else:
            raise FileNotFoundError(
                "Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh GRU t·∫°i c√°c v·ªã tr√≠:\n"
                f"  - {gru_path_keras}\n"
                f"  - {gru_path_h5}\n"
                f"  - {gru_path_fixed}\n"
                "T·∫•t c·∫£ m√¥ h√¨nh ph·∫£i c√≥ s·∫µn ƒë·ªÉ pipeline ho·∫°t ƒë·ªông."
            )

        # Hybrid_CNN_BiLSTM
        hybrid_path_h5 = os.path.join(
            self.base_path, "Hybrid_CNN_BiLSTM", "hybrid_cnn_bilstm_model.h5"
        )
        hybrid_path_keras = os.path.join(
            self.base_path, "Hybrid_CNN_BiLSTM", "checkpoints", "hybrid_cnn_bilstm_model.keras"
        )

        hybrid_path = None
        if os.path.exists(hybrid_path_h5):
            hybrid_path = hybrid_path_h5
            print("  L∆∞u √Ω: S·ª≠ d·ª•ng file .h5 (∆∞u ti√™n ƒë·ªÉ tr√°nh l·ªói encoding)")
        elif os.path.exists(hybrid_path_keras):
            hybrid_path = hybrid_path_keras
            print(
                "  C·∫£nh b√°o: ƒêang load file .keras - c√≥ th·ªÉ g·∫∑p l·ªói encoding tr√™n Windows"
            )

        if hybrid_path:
            required_models.append(("Hybrid_CNN_BiLSTM", hybrid_path, "keras"))
        else:
            raise FileNotFoundError(
                "Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh Hybrid_CNN_BiLSTM t·∫°i c√°c v·ªã tr√≠:\n"
                f"  - {hybrid_path_h5}\n"
                f"  - {hybrid_path_keras}\n"
                "T·∫•t c·∫£ m√¥ h√¨nh ph·∫£i c√≥ s·∫µn ƒë·ªÉ pipeline ho·∫°t ƒë·ªông."
            )

        # BERT
        if PYTORCH_AVAILABLE:
            bert_path = os.path.join(self.base_path, "BERT", "bert_base_email_model")
            if os.path.exists(bert_path) and os.path.exists(
                os.path.join(bert_path, "config.json")
            ):
                required_models.append(("BERT", bert_path, "pytorch"))
            else:
                print(f"  ‚ö† Kh√¥ng t√¨m th·∫•y m√¥ h√¨nh BERT t·∫°i {bert_path}")
                print("  BERT s·∫Ω kh√¥ng ƒë∆∞·ª£c load. C√†i ƒë·∫∑t PyTorch v√† Transformers ƒë·ªÉ s·ª≠ d·ª•ng BERT.")
        else:
            print("  ‚ö† PyTorch kh√¥ng c√≥ s·∫µn. BERT s·∫Ω kh√¥ng ƒë∆∞·ª£c load.")

        # Load t·ª´ng m√¥ h√¨nh
        for model_name, model_path, model_type in required_models:
            try:
                if model_type == "keras":
                    model = self.load_keras_model(model_name, model_path)
                    models_dict[model_name] = {"model": model, "type": "keras"}
                elif model_type == "pytorch":
                    model, tokenizer = self.load_pytorch_model(model_name, model_path)
                    models_dict[model_name] = {
                        "model": model,
                        "tokenizer": tokenizer,
                        "type": "pytorch",
                    }
            except Exception as exc:
                error_msg = (
                    f"\n{'='*80}\n"
                    f"L·ªñI: Kh√¥ng th·ªÉ load m√¥ h√¨nh {model_name}\n"
                    f"{'='*80}\n"
                    f"Pipeline s·∫Ω D·ª™NG HO√ÄN TO√ÄN ƒë·ªÉ ƒë·∫£m b·∫£o t√≠nh nh·∫•t qu√°n.\n"
                    f"T·∫•t c·∫£ m√¥ h√¨nh ph·∫£i load th√†nh c√¥ng ho·∫∑c kh√¥ng ch·∫°y g√¨ c·∫£.\n"
                    f"\nChi ti·∫øt l·ªói:\n{str(exc)}\n"
                    f"{'='*80}\n"
                )
                print(error_msg)
                raise RuntimeError(
                    "Pipeline d·ª´ng: M√¥ h√¨nh "
                    f"{model_name} kh√¥ng th·ªÉ load ƒë∆∞·ª£c. Vui l√≤ng fix l·ªói tr∆∞·ªõc khi ti·∫øp t·ª•c."
                ) from exc

        print(f"\n{'='*80}")
        print(f"‚úì ƒê√É LOAD TH√ÄNH C√îNG T·∫§T C·∫¢ {len(models_dict)} M√î H√åNH")
        print(f"  C√°c m√¥ h√¨nh: {', '.join(models_dict.keys())}")
        print(f"{'='*80}\n")

        self.models = models_dict
        return models_dict
    
    def predict_keras(self, model: tf.keras.Model, email_text: str) -> Tuple[str, float]:
        """
        D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh Keras (BiLSTM, CNN, GRU, Hybrid_CNN_BiLSTM).
        """
        has_text_vectorization = any(
            isinstance(layer, tf.keras.layers.TextVectorization) for layer in model.layers
        )

        if has_text_vectorization:
            email_tensor = tf.convert_to_tensor([email_text], dtype=tf.string)
        else:
            if not hasattr(self, "_shared_text_vectorizer"):
                self._shared_text_vectorizer = tf.keras.layers.TextVectorization(
                    max_tokens=20000,
                    output_mode="int",
                    output_sequence_length=200,
                )

                bilstm_vocab_file = os.path.join(
                    self.base_path, "BiLSTM", "bilstm_embedding_metadata.tsv"
                )
                if os.path.exists(bilstm_vocab_file):
                    import io

                    with io.open(bilstm_vocab_file, "r", encoding="utf-8") as f:
                        words = [line.strip() for line in f if line.strip()]
                    vocabulary = ["", "[UNK]"] + words[:19998]
                    self._shared_text_vectorizer.set_vocabulary(vocabulary)

            email_tensor = self._shared_text_vectorizer([email_text])

        prediction = model.predict(email_tensor, verbose=0)

        if prediction.ndim > 1:
            prob = (
                float(prediction[0][0])
                if prediction.shape[1] == 1
                else float(np.max(prediction[0]))
            )
        else:
            prob = float(prediction[0])

        label = "phishing" if prob > 0.5 else "benign"
        probability = prob if prob > 0.5 else 1 - prob

        return label, probability
    
    def predict_pytorch(
        self, model: Any, tokenizer: Any, email_text: str, model_name: str | None = None
    ) -> Tuple[str, float]:
        """
        D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh PyTorch (BERT).
        """
        if not PYTORCH_AVAILABLE:
            raise ImportError("PyTorch kh√¥ng c√≥ s·∫µn")

        max_length = 512

        inputs = tokenizer(
            email_text,
            padding=True,
            truncation=True,
            max_length=max_length,
            return_tensors="pt",
        )

        device = next(model.parameters()).device
        inputs = {k: v.to(device) for k, v in inputs.items()}

        with torch.no_grad():  # type: ignore[union-attr]
            outputs = model(**inputs)
            logits = outputs.logits
            probabilities = torch.softmax(logits, dim=-1)
            probabilities = probabilities.cpu().numpy()[0]

        prob_benign = float(probabilities[0])
        prob_phishing = float(probabilities[1])

        label = "phishing" if prob_phishing > 0.5 else "benign"
        probability = prob_phishing if prob_phishing > 0.5 else prob_benign

        return label, probability

    def predict(self, model_name: str, email_text: str) -> Tuple[str, float]:
        """
        D·ª± ƒëo√°n v·ªõi m√¥ h√¨nh b·∫•t k·ª≥.
        """
        if model_name not in self.models:
            raise ValueError(
                f"M√¥ h√¨nh {model_name} ch∆∞a ƒë∆∞·ª£c load. H√£y g·ªçi load_all_models() tr∆∞·ªõc."
            )

        model_info = self.models[model_name]

        if model_info["type"] == "keras":
            return self.predict_keras(model_info["model"], email_text)
        if model_info["type"] == "pytorch":
            return self.predict_pytorch(
                model_info["model"],
                model_info["tokenizer"],
                email_text,
                model_name=model_name,
            )

        raise ValueError(f"Lo·∫°i m√¥ h√¨nh kh√¥ng ƒë∆∞·ª£c h·ªó tr·ª£: {model_info['type']}")

